{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KM9XVDmL2SoT"
   },
   "source": [
    "# Demo 10 : Decision ðŸŒ³s \n",
    "\n",
    "In this demo, you will implement:\n",
    "- training a decision tree\n",
    "- different methods for training a random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwafc_T22Soe"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We will use the Iris dataset again. Start by loading the data and splitting it into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 605,
     "status": "error",
     "timestamp": 1571855582193,
     "user": {
      "displayName": "Michael Noukhovitch",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDT3ZsnwHrjcW7fdUa-Mb1TerjtXToPJnrLKo2j=s64",
      "userId": "07518432243701106484"
     },
     "user_tz": 240
    },
    "id": "WlNPne7V2So6",
    "outputId": "41d58b8c-2543-4e6e-a012-163a06bf10ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(iris_train_x, iris_test_x, \n",
    " iris_train_y, iris_test_y) = train_test_split(iris.data, iris.target, test_size=100)\n",
    "\n",
    "print(iris_train_x.shape)\n",
    "print(iris_test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2xbeZdm2So5"
   },
   "source": [
    "## Decision tree\n",
    "\n",
    "To implement the basic decision tree, we will first implement some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_majority_class(labels):\n",
    "    # This function should return the most common label\n",
    "    # in the input array.\n",
    "    pass\n",
    "\n",
    "def compute_entropy(labels):\n",
    "    # This function should compute the entropy\n",
    "    # (= sum_l(-p_l log2 (p_l)) for each label l)\n",
    "    # of the input array.\n",
    "    pass\n",
    "\n",
    "example_labels = np.array([3,1,2,0,2])\n",
    "print(get_majority_class(example_labels)) # should be 2\n",
    "print(compute_entropy(example_labels)) # should be 1.9219280948873623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a **Node** class to use in the decision tree. A node in the tree can:\n",
    "- ask a question,\n",
    "- find the best question to split a (subset of a) dataset, or\n",
    "- if it is a leaf node, predict an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.col = None\n",
    "        self.is_leaf = None\n",
    "        self.output_class = None\n",
    "        \n",
    "    def find_best_question(self, x, y):\n",
    "        # x: np array of shape (number of examples, number of features)\n",
    "        # y: np array of shape (number of examples,)\n",
    "        best_col = None\n",
    "        best_val = None\n",
    "        best_loss = np.inf\n",
    "        \n",
    "        num_cols = x.shape[1]\n",
    "        valid_cols = np.arange(num_cols)\n",
    "        for col in valid_cols:\n",
    "            #\n",
    "            # Compute the midpoints of this column's values here\n",
    "            #\n",
    "            \n",
    "            for val in midpoints:\n",
    "                #\n",
    "                # Using col and val, split the labels\n",
    "                # into left_labels, right_labels here\n",
    "                #\n",
    "                \n",
    "                right_entropy = compute_entropy(right_labels)\n",
    "                left_entropy = compute_entropy(left_labels)\n",
    "                loss = left_entropy + right_entropy\n",
    "                if right_labels.shape[0] == 0 or left_labels.shape[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_col = col\n",
    "                    best_val = val\n",
    "                    \n",
    "        self.col = best_col\n",
    "        self.threshold = best_val\n",
    "    \n",
    "    def ask_question(self, x):\n",
    "        if not self.is_leaf:\n",
    "            return x[:, self.col] > self.threshold\n",
    "        else:\n",
    "            print(\"Error: leaf nodes cannot ask questions!\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self):\n",
    "        if self.is_leaf:\n",
    "            return self.output_class\n",
    "        else:\n",
    "            print(\"Error: non-leaf nodes cannot make a prediction!\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the Node class, we will implement the decision tree. Like scikit-learn's interface, we will have fit() and predict() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "XoqPgAM72So_"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def create_node(self, x_subset, y_subset, depth):\n",
    "        # Recursive function\n",
    "        node = Node()\n",
    "        \n",
    "        majority_class = get_majority_class(y_subset)\n",
    "        majority_class_count = (y_subset == majority_class).sum()\n",
    "        perfectly_classified = majority_class_count == len(y_subset)\n",
    "        \n",
    "        if perfectly_classified or depth == self.max_depth:\n",
    "            node.output_class = majority_class\n",
    "            node.is_leaf = True\n",
    "        else:\n",
    "            node.find_best_question(x_subset,y_subset)\n",
    "            node.is_leaf = False\n",
    "            right_subset_rows = node.ask_question(x_subset) \n",
    "            left_subset_rows = np.invert(right_subset_rows)\n",
    "            #\n",
    "            # Recursion: create node.left_child and node.right_child here\n",
    "            #\n",
    "                \n",
    "        return node\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.root_node = self.create_node(x,y,depth=1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            current_node = self.root_node\n",
    "            x_i = x[i].reshape(1,-1)\n",
    "            done_descending_tree = False\n",
    "            while not done_descending_tree:\n",
    "                if current_node.is_leaf:\n",
    "                    predictions.append(current_node.predict())\n",
    "                    done_descending_tree = True\n",
    "\n",
    "                else:\n",
    "                    if current_node.ask_question(x_i):\n",
    "                        current_node = current_node.right_child\n",
    "                    else:\n",
    "                        current_node = current_node.left_child\n",
    "\n",
    "        return np.array(predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a tree to test your implementation. (You can also try comparing with scikit-learn's tree implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.tree\n",
    "# tree = sklearn.tree.DecisionTreeClassifier(max_depth=10)\n",
    "tree = DecisionTreeClassifier(max_depth=10)\n",
    "tree.fit(iris_train_x, iris_train_y)\n",
    "print(iris_test_y)\n",
    "print(tree.predict(iris_test_x))\n",
    "print(\"accuracy: \", (tree.predict(iris_test_x) == iris_test_y).mean() * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train trees of increasing depth on the Iris dataset and plot their performance. What do you notice about the performance as the depth increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "accuracies = []\n",
    "for depth in range(1, 20):\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    model.fit(iris_train_x, iris_train_y)\n",
    "    accuracy = (model.predict(iris_test_x) == iris_test_y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "    depths.append(depth)\n",
    "\n",
    "print(\"Best performance: \", max(accuracies))\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Performance on test set')\n",
    "plt.plot(depths, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2xbeZdm2So5"
   },
   "source": [
    "## Random forest\n",
    "\n",
    "Next, we will implement the random forest model using our decision tree. \n",
    "\n",
    "A random forest is an ensemble of decision trees, where each tree is trained by randomizing the dataset in some way.\n",
    "\n",
    "We will explore two ways of randomizing the dataset: 1) **bagging**, i.e. training each classifier on a random subset of the dataset, and 2) **adding Gaussian noise** to the features of the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier():\n",
    "    def __init__(self, n_estimators=2, max_depth=5, subset_percentage=0.5, noise_variance=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subset_percentage = subset_percentage\n",
    "        self.noise_variance = noise_variance\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        num_rows = round(self.subset_percentage * len(x))\n",
    "        for _ in range(self.n_estimators):\n",
    "            # \n",
    "            # Create noisy subsets x_subset, y_subset here\n",
    "            # \n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(x_subset, y_subset)\n",
    "            self.estimators.append(tree)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        #\n",
    "        # Predict output using all estimators here\n",
    "        # \n",
    "        \n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot the performance as a function of the number of trees when using bagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1571854847206,
     "user": {
      "displayName": "RÃ©mi LP",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDCHZM0ovaKL5mFEhgC-br1CQgfekn1p9JsJsr2NA=s64",
      "userId": "02691211833008544468"
     },
     "user_tz": 240
    },
    "id": "My2jcUPk2SpA",
    "outputId": "c3332b13-4b6b-40e1-9260-02160b87a530",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_counts = []\n",
    "accuracies = []\n",
    "for tree_count in range(1, 20):\n",
    "    model = RandomForestClassifier(n_estimators=tree_count,\n",
    "                                   max_depth=3,\n",
    "                                   subset_percentage=0.5,\n",
    "                                   noise_variance=0)\n",
    "    model.fit(iris_train_x, iris_train_y)\n",
    "    accuracy = (model.predict(iris_test_x) == iris_test_y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "    tree_counts.append(tree_count)\n",
    "\n",
    "print(\"Best performance: \", max(accuracies))\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Performance on test set')\n",
    "plt.plot(tree_counts, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FdVoYbI2SpP"
   },
   "source": [
    "Next, let's try adding Gaussian noise. (What happens if you set the noise variance to 0? Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nPtGKtd2SpQ",
    "outputId": "cf0ec137-8001-4166-b1e0-7bc56c1dfc26"
   },
   "outputs": [],
   "source": [
    "tree_counts = []\n",
    "accuracies = []\n",
    "for tree_count in range(1, 20):\n",
    "    model = RandomForestClassifier(n_estimators=tree_count,\n",
    "                                   max_depth=3,\n",
    "                                   subset_percentage=1,\n",
    "                                   noise_variance=0.1)\n",
    "    model.fit(iris_train_x, iris_train_y)\n",
    "    accuracy = (model.predict(iris_test_x) == iris_test_y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "    tree_counts.append(tree_count)\n",
    "\n",
    "print(\"Best performance: \", max(accuracies))\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Performance on test set')\n",
    "plt.plot(tree_counts, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "9 - (Completed) sklearn-library.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
