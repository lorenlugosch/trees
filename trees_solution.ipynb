{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KM9XVDmL2SoT"
   },
   "source": [
    "# Demo 10 : Decision ðŸŒ³s \n",
    "\n",
    "In this demo, you will implement:\n",
    "- training a decision tree\n",
    "- different methods for training a random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwafc_T22Soe"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We will use the Iris dataset again. Start by loading the data and splitting it into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 605,
     "status": "error",
     "timestamp": 1571855582193,
     "user": {
      "displayName": "Michael Noukhovitch",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDT3ZsnwHrjcW7fdUa-Mb1TerjtXToPJnrLKo2j=s64",
      "userId": "07518432243701106484"
     },
     "user_tz": 240
    },
    "id": "WlNPne7V2So6",
    "outputId": "41d58b8c-2543-4e6e-a012-163a06bf10ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Split into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(iris_train_x, iris_test_x, \n",
    " iris_train_y, iris_test_y) = train_test_split(iris.data, iris.target, test_size=100)\n",
    "\n",
    "print(iris_train_x.shape)\n",
    "print(iris_test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2xbeZdm2So5"
   },
   "source": [
    "## Decision tree\n",
    "\n",
    "To implement the basic decision tree, we will first implement some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_majority_class(labels):\n",
    "    counts = Counter(labels)\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "def compute_entropy(labels):\n",
    "    counts = Counter(labels)\n",
    "    counts = np.array([counts[key] for key in counts])\n",
    "    frequencies = counts/counts.sum()\n",
    "    entropy = -(frequencies * np.log2(frequencies)).sum()\n",
    "    return entropy\n",
    "\n",
    "example_labels = np.array([3,1,2,0,2])\n",
    "print(get_majority_class(example_labels)) # should be 2\n",
    "print(compute_entropy(example_labels)) # should be 1.9219280948873623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a **Node** class to use in the decision tree. A node in the tree can:\n",
    "- ask a question,\n",
    "- find the best question to split a (subset of a) dataset, or\n",
    "- if it is a leaf node, predict an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "        self.col = None\n",
    "        self.is_leaf = None\n",
    "        self.output_class = None\n",
    "        \n",
    "    def find_best_question(self, x, y):\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        exit\n",
    "        best_col = None\n",
    "        best_val = None\n",
    "        best_loss = np.inf\n",
    "        \n",
    "        num_cols = x.shape[1]\n",
    "        valid_cols = np.arange(num_cols)\n",
    "        for col in valid_cols:\n",
    "            sorted_indices = x[:, col].argsort()\n",
    "            sorted_vals = x[sorted_indices, col]\n",
    "            midpoints = [(sorted_vals[i] + sorted_vals[i+1])/2 for i in range(len(sorted_vals)-1) ]\n",
    "            for val in midpoints:\n",
    "                right_subset_rows = x[:, col] > val \n",
    "                left_subset_rows = np.invert(right_subset_rows)\n",
    "                right_labels = y[right_subset_rows]\n",
    "                left_labels = y[left_subset_rows]\n",
    "                \n",
    "                right_entropy = compute_entropy(right_labels)\n",
    "                left_entropy = compute_entropy(left_labels)\n",
    "                loss = left_entropy + right_entropy\n",
    "                if right_labels.shape[0] == 0 or left_labels.shape[0] == 0:\n",
    "                    continue\n",
    "                \n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_col = col\n",
    "                    best_val = val\n",
    "                    \n",
    "        self.col = best_col\n",
    "        self.threshold = best_val\n",
    "    \n",
    "    def ask_question(self, x):\n",
    "        if not self.is_leaf:\n",
    "            return x[:, self.col] > self.threshold\n",
    "        else:\n",
    "            print(\"Error: leaf nodes cannot ask questions!\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self):\n",
    "        if self.is_leaf:\n",
    "            return self.output_class\n",
    "        else:\n",
    "            print(\"Error: non-leaf nodes cannot make a prediction!\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the Node class, we will implement the decision tree. Like scikit-learn's interface, we will have fit() and predict() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "XoqPgAM72So_"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def create_node(self, x_subset, y_subset, depth):\n",
    "        # Recursive function\n",
    "        node = Node()\n",
    "        \n",
    "        majority_class = get_majority_class(y_subset)\n",
    "        majority_class_count = (y_subset == majority_class).sum()\n",
    "        perfectly_classified = majority_class_count == len(y_subset)\n",
    "        \n",
    "        if perfectly_classified or depth == self.max_depth:\n",
    "            node.output_class = majority_class\n",
    "            node.is_leaf = True\n",
    "        else:\n",
    "            node.find_best_question(x_subset,y_subset)\n",
    "            node.is_leaf = False\n",
    "            right_subset_rows = node.ask_question(x_subset) \n",
    "            left_subset_rows = np.invert(right_subset_rows)\n",
    "            node.left_child = self.create_node(x_subset[left_subset_rows], y_subset[left_subset_rows],  depth+1)\n",
    "            node.right_child = self.create_node(x_subset[right_subset_rows], y_subset[right_subset_rows], depth+1)\n",
    "                \n",
    "        return node\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.root_node = self.create_node(x,y,depth=1)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            current_node = self.root_node\n",
    "            x_i = x[i].reshape(1,-1)\n",
    "            done_descending_tree = False\n",
    "            while not done_descending_tree:\n",
    "                if current_node.is_leaf:\n",
    "                    predictions.append(current_node.predict())\n",
    "                    done_descending_tree = True\n",
    "\n",
    "                else:\n",
    "                    if current_node.ask_question(x_i):\n",
    "                        current_node = current_node.right_child\n",
    "                    else:\n",
    "                        current_node = current_node.left_child\n",
    "\n",
    "        return np.array(predictions)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a tree to test your implementation. (You can also try comparing with scikit-learn's tree implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.tree\n",
    "# tree = sklearn.tree.DecisionTreeClassifier(max_depth=10)\n",
    "tree = DecisionTreeClassifier(max_depth=10)\n",
    "tree.fit(iris_train_x, iris_train_y)\n",
    "print(iris_test_y)\n",
    "print(tree.predict(iris_test_x))\n",
    "print(\"accuracy: \", (tree.predict(iris_test_x) == iris_test_y).mean() * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train trees of increasing depth on the Iris dataset and plot their performance. What do you notice about the performance as the depth increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "accuracies = []\n",
    "for depth in range(1, 20):\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    model.fit(iris_train_x, iris_train_y)\n",
    "    accuracy = (model.predict(iris_test_x) == iris_test_y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "    depths.append(depth)\n",
    "\n",
    "print(\"Best performance: \", max(accuracies))\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('Performance on test set')\n",
    "plt.plot(depths, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2xbeZdm2So5"
   },
   "source": [
    "## Random forest\n",
    "\n",
    "Next, we will implement the random forest model using our decision tree. \n",
    "\n",
    "A random forest is an ensemble of decision trees, where each tree is trained by randomizing the dataset in some way.\n",
    "\n",
    "We will explore two ways of randomizing the dataset: 1) **bagging**, i.e. training each classifier on a random subset of the dataset, and 2) **adding Gaussian noise** to the features of the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier():\n",
    "    def __init__(self, n_estimators=2, max_depth=5, subset_percentage=0.5, noise_variance=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.n_estimators = n_estimators\n",
    "        self.subset_percentage = subset_percentage\n",
    "        self.noise_variance = noise_variance\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        num_rows = round(self.subset_percentage * len(x))\n",
    "        for _ in range(self.n_estimators):\n",
    "            rows = np.random.choice(x.shape[0], num_rows, replace=False)\n",
    "            x_subset = x[rows] + self.noise_variance*np.random.randn(x[rows].shape[0], x[rows].shape[1])\n",
    "            y_subset = y[rows]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(x_subset, y_subset)\n",
    "            self.estimators.append(tree)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        all_predictions = []\n",
    "        for tree in self.estimators:\n",
    "            all_predictions.append(tree.predict(x))\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(x)):\n",
    "            predictions.append(get_majority_class(all_predictions[:,i]))\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot the performance as a function of the number of trees when using bagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 791,
     "status": "ok",
     "timestamp": 1571854847206,
     "user": {
      "displayName": "RÃ©mi LP",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDCHZM0ovaKL5mFEhgC-br1CQgfekn1p9JsJsr2NA=s64",
      "userId": "02691211833008544468"
     },
     "user_tz": 240
    },
    "id": "My2jcUPk2SpA",
    "outputId": "c3332b13-4b6b-40e1-9260-02160b87a530",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_counts = []\n",
    "accuracies = []\n",
    "for tree_count in range(1, 20):\n",
    "    model = RandomForestClassifier(n_estimators=tree_count,\n",
    "                                   max_depth=3,\n",
    "                                   subset_percentage=0.5,\n",
    "                                   noise_variance=0)\n",
    "    model.fit(iris_train_x, iris_train_y)\n",
    "    accuracy = (model.predict(iris_test_x) == iris_test_y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "    tree_counts.append(tree_count)\n",
    "\n",
    "print(\"Best performance: \", max(accuracies))\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Performance on test set')\n",
    "plt.plot(tree_counts, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FdVoYbI2SpP"
   },
   "source": [
    "Next, let's try adding Gaussian noise. (What happens if you set the noise variance to 0? Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nPtGKtd2SpQ",
    "outputId": "cf0ec137-8001-4166-b1e0-7bc56c1dfc26"
   },
   "outputs": [],
   "source": [
    "tree_counts = []\n",
    "accuracies = []\n",
    "for tree_count in range(1, 20):\n",
    "    model = RandomForestClassifier(n_estimators=tree_count,\n",
    "                                   max_depth=3,\n",
    "                                   subset_percentage=1,\n",
    "                                   noise_variance=0.1)\n",
    "    model.fit(iris_train_x, iris_train_y)\n",
    "    accuracy = (model.predict(iris_test_x) == iris_test_y).mean()\n",
    "    accuracies.append(accuracy)\n",
    "    tree_counts.append(tree_count)\n",
    "\n",
    "print(\"Best performance: \", max(accuracies))\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Performance on test set')\n",
    "plt.plot(tree_counts, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "9 - (Completed) sklearn-library.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
